{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Steven Binder, The Photonics and Soft Robotics Lab, The University of Georgia\n",
    "Date:   08-05-2025\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, os.path\n",
    "import warnings \n",
    "import math\n",
    "import librosa\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense,concatenate, Conv2D, Add, BatchNormalization,SpatialDropout2D, Dropout, Flatten, GlobalAveragePooling2D,MaxPooling2D\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix,accuracy_score,f1_score,roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_features_and_labels(data,labels):\n",
    "    samples,time,feat= data.shape\n",
    "    reshaped_data = np.zeros((samples * 3, time, 2), dtype=data.dtype)\n",
    "\n",
    "    reshaped_data[0::3, :, 0]=data[:, :, 0]  \n",
    "    reshaped_data[0::3, :, 1]=data[:, :, 1]  \n",
    "    reshaped_data[1::3, :, 0]=data[:, :, 2]  \n",
    "    reshaped_data[1::3, :, 1]=data[:, :, 3]  \n",
    "    reshaped_data[2::3, :, 0] = data[:, :, 4]  \n",
    "    reshaped_data[2::3, :, 1] = data[:, :, 5] \n",
    "\n",
    "    reshaped_labels = np.repeat(labels, 3)\n",
    "\n",
    "    return reshaped_data, reshaped_labels\n",
    "\n",
    "def compute_mel_spectrogram(data, sample_rate, n_mels, n_fft, hop_length):\n",
    "    samples, time, features = data.shape\n",
    "\n",
    "    spectrograms = np.zeros((samples, n_mels, time, features))\n",
    "\n",
    "    for i in range(samples):\n",
    "        for j in range(features):\n",
    "            time_series = data[i, :, j]\n",
    "            if np.any(np.isnan(time_series)):\n",
    "                print(f\"Warning: NaN values found in sample {i}, feature {j}\")\n",
    "                time_series = np.nan_to_num(time_series)\n",
    "            if np.max(np.abs(time_series)) > 0:\n",
    "                time_series = time_series / np.max(np.abs(time_series))\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=time_series,\n",
    "                sr=sample_rate,\n",
    "                n_mels=n_mels,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                power=2.0\n",
    "            )\n",
    "            mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            spectrograms[i, :, :, j] = mel_db\n",
    "\n",
    "    return spectrograms\n",
    "\n",
    "def augment_spectrogram(data,gaussian_noise_prob=0.8, gaussian_noise_std=0.1,\n",
    "                       multiplicative_noise_prob=0.5, multiplicative_noise_range=(0.7, 1.3),\n",
    "                       time_shift_prob=0.3, time_shift_max=20):\n",
    "    augmented_data = np.copy(data)\n",
    "    if np.random.random() < gaussian_noise_prob: # Adding Gaussian noise\n",
    "        noise = np.random.normal(0, gaussian_noise_std, size=data.shape)\n",
    "        augmented_data = augmented_data + noise\n",
    "\n",
    "    if np.random.random() < multiplicative_noise_prob: # Adding multiplicative noise\n",
    "        for c in range(data.shape[2]):\n",
    "            scale_factor = np.random.uniform(multiplicative_noise_range[0],multiplicative_noise_range[1])\n",
    "            augmented_data[:, :, c] = augmented_data[:, :, c] * scale_factor\n",
    "\n",
    "    if np.random.random() < time_shift_prob: # Adding time shift\n",
    "        shift_amount = np.random.randint(-time_shift_max, time_shift_max + 1)\n",
    "        if shift_amount != 0:\n",
    "            for c in range(data.shape[2]):\n",
    "                if shift_amount > 0:  \n",
    "                    augmented_data[:, shift_amount:, c] = augmented_data[:, :-shift_amount, c]\n",
    "                    augmented_data[:, :shift_amount, c] = 0\n",
    "                else: \n",
    "                    shift_amount = abs(shift_amount)\n",
    "                    augmented_data[:, :-shift_amount, c] = augmented_data[:, shift_amount:, c]\n",
    "                    augmented_data[:, -shift_amount:, c] = 0\n",
    "    \n",
    "    augmented_data = np.clip(augmented_data, 0, 1)\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and validation data\n",
    "\n",
    "titles=['D01X.npy','D01Y.npy','D02X.npy','D02Y.npy','D03X.npy','D03Y.npy','D04X.npy','D04Y.npy','D05X.npy','D05Y.npy','D06X.npy','D06Y.npy',\n",
    "        'D07X.npy','D07Y.npy','D08X.npy','D08Y.npy','D09X.npy','D09Y.npy','D10X.npy','D10Y.npy',\n",
    "        'D01X_100.npy','D01Y_100.npy','D02X_100.npy','D02Y_100.npy','D03X_100.npy','D03Y_100.npy','D04X_100.npy','D04Y_100.npy',\n",
    "        'D05X_100.npy','D05Y_100.npy','D06X_100.npy','D06Y_100.npy','D07X_100.npy','D07Y_100.npy','D08X_100.npy','D08Y_100.npy','D09X_100.npy','D09Y_100.npy','D10X_100.npy','D10Y_100.npy',\n",
    "        'D01X_300.npy','D01Y_300.npy','D02X_300.npy','D02Y_300.npy','D03X_300.npy','D03Y_300.npy','D04X_300.npy','D04Y_300.npy',\n",
    "        'D05X_300.npy','D05Y_300.npy','D06X_300.npy','D06Y_300.npy','D07X_300.npy','D07Y_300.npy','D08X_300.npy','D08Y_300.npy','D09X_300.npy','D09Y_300.npy','D10X_300.npy','D10Y_300.npy',\n",
    "        'D01X_750.npy','D01Y_750.npy','D02X_750.npy','D02Y_750.npy','D03X_750.npy','D03Y_750.npy','D04X_750.npy','D04Y_750.npy',\n",
    "        'D05X_750.npy','D05Y_750.npy','D06X_750.npy','D06Y_750.npy','D07X_750.npy','D07Y_750.npy','D08X_750.npy','D08Y_750.npy','D09X_750.npy','D09Y_750.npy','D10X_750.npy','D10Y_750.npy',\n",
    "        'D01X_1000.npy','D01Y_1000.npy','D02X_1000.npy','D02Y_1000.npy','D03X_1000.npy','D03Y_1000.npy','D04X_1000.npy','D04Y_1000.npy',\n",
    "        'D05X_1000.npy','D05Y_1000.npy','D06X_1000.npy','D06Y_1000.npy','D07X_1000.npy','D07Y_1000.npy','D08X_1000.npy','D08Y_1000.npy','D09X_1000.npy','D09Y_1000.npy','D10X_1000.npy','D10Y_1000.npy'\n",
    "        ]\n",
    "\n",
    "\n",
    "sam_rate = 5000  \n",
    "n_mels =128\n",
    "n_fft = 1024     \n",
    "hop_length =512\n",
    "\n",
    "batch_size = 10\n",
    "all_processed_X = []\n",
    "all_processed_Y = []\n",
    "\n",
    "for i in range(0, len(titles), batch_size):\n",
    "    batch_titles = titles[i:i+batch_size]\n",
    "    k = 0\n",
    "    total_X0, total_X1, total_X2 = None, None, None\n",
    "    for file in batch_titles:\n",
    "        if 'X' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                x0 = np.load(f)#[0:4]\n",
    "                x1 = np.load(f)#[0:4] \n",
    "                x2 = np.load(f)#[0:4]\n",
    "                if 'X_300' in file:\n",
    "                    x0 = x0[:,40000:190000,:]\n",
    "                    x1 = x1[:,40000:190000,:]\n",
    "                    x2 = x2[:,40000:190000,:]\n",
    "                if k == 0:\n",
    "                    total_X0 = x0\n",
    "                    total_X1 = x1\n",
    "                    total_X2 = x2\n",
    "                    k += 1\n",
    "                else:\n",
    "                    total_X0 = np.vstack((total_X0, x0))\n",
    "                    total_X1 = np.vstack((total_X1, x1))\n",
    "                    total_X2 = np.vstack((total_X2, x2))\n",
    "                del x0, x1, x2\n",
    "        if 'Y' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                y0 = np.load(f)\n",
    "                y1 = np.load(f)\n",
    "                y2 = np.load(f)\n",
    "                if 'Y_300' in file:\n",
    "                    y0 = y0[40000:190000]\n",
    "                    y1 = y1[40000:190000]\n",
    "                    y2 = y2[40000:190000]\n",
    "    batch_total_X = []\n",
    "    batch_total_Y = []\n",
    "    if total_X0 is not None:\n",
    "        for i in range(total_X0.shape[0]):\n",
    "            batch_total_X.append(total_X0[i,:,:])\n",
    "            batch_total_Y.append(y0[0])\n",
    "\n",
    "        for i in range(total_X1.shape[0]):\n",
    "            batch_total_X.append(total_X1[i,:,:])\n",
    "            batch_total_Y.append(y1[0])\n",
    "\n",
    "        for i in range(total_X2.shape[0]):\n",
    "            batch_total_X.append(total_X2[i,:,:])\n",
    "            batch_total_Y.append(y2[0])\n",
    "\n",
    "    batch_total_X = np.array(batch_total_X)\n",
    "    batch_total_Y = np.array(batch_total_Y)\n",
    "\n",
    "    batch_total_X, batch_total_Y = reshape_features_and_labels(batch_total_X, batch_total_Y)\n",
    "\n",
    "    batch_total_X = compute_mel_spectrogram(batch_total_X, sample_rate=sam_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    all_processed_X.append(batch_total_X)\n",
    "    all_processed_Y.extend(batch_total_Y)\n",
    "\n",
    "    del batch_total_X, batch_total_Y\n",
    "    del total_X0, total_X1, total_X2\n",
    "    gc.collect()\n",
    "\n",
    "final_X = np.concatenate(all_processed_X, axis=0)\n",
    "final_Y = np.array(all_processed_Y)\n",
    "\n",
    "globMax= np.max(final_X)\n",
    "globMin= np.min(final_X)\n",
    "\n",
    "final_X=(final_X - globMin) / (globMax - globMin)\n",
    "print(final_X.shape,final_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "titles=['D11X.npy','D11Y.npy',\n",
    "        'D11X_100.npy','D11Y_100.npy',\n",
    "        'D11X_300.npy','D11Y_300.npy',\n",
    "        'D11X_750.npy','D11Y_750.npy',\n",
    "        'D11X_1000.npy','D11Y_1000.npy',]\n",
    "\n",
    "batch_size = 10\n",
    "all_processed_X = []\n",
    "all_processed_Y = []\n",
    "\n",
    "for i in range(0, len(titles), batch_size):\n",
    "    batch_titles = titles[i:i+batch_size]\n",
    "    k = 0\n",
    "    total_X0, total_X1, total_X2 = None, None, None\n",
    "    for file in batch_titles:\n",
    "        if 'X' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                x0 = np.load(f)#[0:4]\n",
    "                x1 = np.load(f)#[0:4] \n",
    "                x2 = np.load(f)#[0:4]\n",
    "                if 'X_300' in file:\n",
    "                    x0 = x0[:,40000:190000,:]\n",
    "                    x1 = x1[:,40000:190000,:]\n",
    "                    x2 = x2[:,40000:190000,:]\n",
    "                if k == 0:\n",
    "                    total_X0 = x0\n",
    "                    total_X1 = x1\n",
    "                    total_X2 = x2\n",
    "                    k += 1\n",
    "                else:\n",
    "                    total_X0 = np.vstack((total_X0, x0))\n",
    "                    total_X1 = np.vstack((total_X1, x1))\n",
    "                    total_X2 = np.vstack((total_X2, x2))\n",
    "                del x0, x1, x2\n",
    "        if 'Y' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                y0 = np.load(f)\n",
    "                y1 = np.load(f)\n",
    "                y2 = np.load(f)\n",
    "                if 'Y_300' in file:\n",
    "                    y0 = y0[40000:190000]\n",
    "                    y1 = y1[40000:190000]\n",
    "                    y2 = y2[40000:190000]\n",
    "    batch_total_X = []\n",
    "    batch_total_Y = []\n",
    "\n",
    "    if total_X0 is not None:\n",
    "        for i in range(total_X0.shape[0]):\n",
    "            batch_total_X.append(total_X0[i,:,:])\n",
    "            batch_total_Y.append(y0[0])\n",
    "\n",
    "        for i in range(total_X1.shape[0]):\n",
    "            batch_total_X.append(total_X1[i,:,:])\n",
    "            batch_total_Y.append(y1[0])\n",
    "\n",
    "        for i in range(total_X2.shape[0]):\n",
    "            batch_total_X.append(total_X2[i,:,:])\n",
    "            batch_total_Y.append(y2[0])\n",
    "\n",
    "    batch_total_X = np.array(batch_total_X)\n",
    "    batch_total_Y = np.array(batch_total_Y)\n",
    "\n",
    "    batch_total_X, batch_total_Y = reshape_features_and_labels(batch_total_X, batch_total_Y)\n",
    "\n",
    "    batch_total_X = compute_mel_spectrogram(batch_total_X, sample_rate=sam_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    all_processed_X.append(batch_total_X)\n",
    "    all_processed_Y.extend(batch_total_Y)\n",
    "\n",
    "    del batch_total_X, batch_total_Y\n",
    "    del total_X0, total_X1, total_X2\n",
    "    gc.collect()\n",
    "\n",
    "valid_X = np.concatenate(all_processed_X, axis=0)\n",
    "valid_Y = np.array(all_processed_Y)\n",
    "\n",
    "globMax= np.max(valid_X)\n",
    "globMin= np.min(valid_X)\n",
    "\n",
    "valid_X=(valid_X - globMin) / (globMax - globMin)\n",
    "print(valid_X.shape,valid_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset creation\n",
    "\n",
    "final_X = final_X.astype(np.float32)\n",
    "valid_X = valid_X.astype(np.float32)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_X, final_Y, test_size=0.3, stratify=final_Y)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Augment training data\n",
    "chunk_size = 1000\n",
    "noise_X_train = []\n",
    "\n",
    "for aug_type in range(3):\n",
    "    aug_samples = np.zeros_like(X_train)\n",
    "    \n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        chunk = X_train[i:i+chunk_size].copy()\n",
    "\n",
    "        for j in range(len(chunk)):\n",
    "            if aug_type == 0:\n",
    "                aug_samples[i+j] = augment_spectrogram(chunk[j],gaussian_noise_prob=1.0, gaussian_noise_std=0.12,multiplicative_noise_prob=0.0,time_shift_prob=0.0)\n",
    "            elif aug_type == 1:\n",
    "                aug_samples[i+j] = augment_spectrogram(chunk[j],gaussian_noise_prob=0.3, gaussian_noise_std=0.05,multiplicative_noise_prob=1.0, multiplicative_noise_range=(0.8, 1.2), time_shift_prob=0.0)\n",
    "            else:\n",
    "                aug_samples[i+j] = augment_spectrogram(chunk[j],gaussian_noise_prob=0.3, gaussian_noise_std=0.05,multiplicative_noise_prob=0.3,time_shift_prob=1.0, time_shift_max=15)\n",
    "\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    noise_X_train.append(aug_samples)\n",
    "\n",
    "aug_data = np.concatenate(noise_X_train, axis=0)\n",
    "aug_labels = np.tile(y_train, 3)\n",
    "\n",
    "X_train = np.concatenate((X_train, aug_data), axis=0)\n",
    "y_train = np.concatenate((y_train, aug_labels), axis=0)\n",
    "\n",
    "del noise_X_train, aug_data, aug_labels\n",
    "gc.collect()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "train_DS = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "    .cache()\\\n",
    "    .shuffle(1000)\\\n",
    "    .batch(batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_DS = tf.data.Dataset.from_tensor_slices((X_test, y_test))\\\n",
    "    .cache()\\\n",
    "    .batch(batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_DS = tf.data.Dataset.from_tensor_slices((valid_X, valid_Y))\\\n",
    "    .cache()\\\n",
    "    .batch(batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "del X_train, X_test\n",
    "gc.collect()\n",
    "\n",
    "process = psutil.Process()\n",
    "print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b52a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "drop=0.2\n",
    "reg=0.0001\n",
    "input_shape=(128,293,2) \n",
    "kernel_shape=3\n",
    "layers=[32,64,128,256,512]\n",
    "\n",
    "def model1(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(layers[0], (kernel_shape, kernel_shape), activation='relu', padding='same', kernel_regularizer=l2(reg), input_shape=input_shape))\n",
    "    model.add(Conv2D(layers[0], (kernel_shape, kernel_shape), activation='relu', padding='same', kernel_regularizer=l2(reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(SpatialDropout2D(drop))\n",
    "\n",
    "    model.add(Conv2D(layers[1], (kernel_shape, kernel_shape), activation='relu', padding='same', kernel_regularizer=l2(reg), input_shape=input_shape))\n",
    "    model.add(Conv2D(layers[1], (kernel_shape, kernel_shape), activation='relu', padding='same', kernel_regularizer=l2(reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(SpatialDropout2D(drop))\n",
    "\n",
    "    model.add(Conv2D(layers[2], (kernel_shape, kernel_shape), activation='relu', padding='same', kernel_regularizer=l2(reg), input_shape=input_shape))\n",
    "    model.add(Conv2D(layers[2], (kernel_shape, kernel_shape), activation='relu', padding='same', kernel_regularizer=l2(reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(SpatialDropout2D(drop))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    return model\n",
    "\n",
    "def res_block(input_ten,filters,strides=(1,1)):\n",
    "    x=Conv2D(filters,(kernel_shape,kernel_shape),padding='same',kernel_regularizer=l2(reg))(input_ten)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=tf.keras.layers.Activation('relu')(x)\n",
    "    x=Conv2D(filters,(kernel_shape,kernel_shape),padding='same',kernel_regularizer=l2(reg))(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    if strides!=(1,1):\n",
    "        input_ten=Conv2D(filters,(1,1),padding='same',kernel_regularizer=l2(reg))(input_ten)\n",
    "        input_ten=BatchNormalization()(input_ten)\n",
    "    x=Add()([x,input_ten])\n",
    "    x=tf.keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def model2(input_shape):\n",
    "    input_ten=Input(shape=input_shape)\n",
    "    x=Conv2D(layers[0],(7,7),strides=(2,2),padding='same',kernel_regularizer=l2(reg))(input_ten)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=tf.keras.layers.Activation('relu')(x)\n",
    "    x=MaxPooling2D((3,3),strides=(2,2),padding='same')(x)\n",
    "\n",
    "    x=res_block(x,layers[0])\n",
    "    x=res_block(x,layers[0])\n",
    "\n",
    "    x=res_block(x,layers[1],strides=(2,2))\n",
    "    x=res_block(x,layers[1])\n",
    "\n",
    "    x=res_block(x,layers[2],strides=(2,2))\n",
    "    x=res_block(x,layers[2])\n",
    "\n",
    "    x=res_block(x,layers[3],strides=(2,2))\n",
    "    x=res_block(x,layers[3])\n",
    "\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "\n",
    "    model=Model(input_ten,x)\n",
    "    return model\n",
    "\n",
    "def combo_model(input_shape):\n",
    "    input_sig=Input(shape=input_shape)\n",
    "\n",
    "    mod1=model1(input_shape)\n",
    "    mod1_Out=mod1(input_sig)\n",
    "\n",
    "    mod2=model2(input_shape)\n",
    "    mod2_Out=mod2(input_sig)\n",
    "\n",
    "    concat_features=concatenate([mod1_Out,mod2_Out])\n",
    "\n",
    "    x=Dense(layers[4],activation='relu')(concat_features)\n",
    "    x=Dropout(drop)(x)\n",
    "    x=Dense(layers[3],activation='relu')(x)\n",
    "    x=Dropout(drop)(x)\n",
    "    x=Dense(layers[2],activation='relu')(x)\n",
    "    x=Dropout(drop)(x)\n",
    "    x=Dense(layers[1],activation='relu')(x)\n",
    "    x=Dropout(drop)(x)\n",
    "    x=Dense(layers[0],activation='relu')(x)\n",
    "    x=Dropout(drop)(x)\n",
    "    x=Dense(3,activation='softmax')(x)\n",
    "\n",
    "    model=Model(input_sig,x)\n",
    "    return model\n",
    "\n",
    "model=combo_model(input_shape)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.00001\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 20.0\n",
    "    lr = initial_lr * math.pow(drop_rate, math.floor((1+epoch)/epochs_drop))\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "checkpoint=ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001, clipnorm=1.0, use_ema=True),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "r = model.fit(train_DS,validation_data=(valid_DS),epochs=150,callbacks=[lr_scheduler,checkpoint])  \n",
    "\n",
    "# View loss and accuracy curves\n",
    "sns.set(rc={'figure.figsize':(20,5)})\n",
    "fig,ax=plt.subplots(1,2)\n",
    "sns.lineplot(x=range(len(r.history['loss'])), y=r.history['loss'], label='loss', ax=ax[0])\n",
    "sns.lineplot(x=range(len(r.history['val_loss'])), y=r.history['val_loss'], label='val_loss', ax=ax[0])\n",
    "sns.lineplot(x=range(len(r.history['accuracy'])), y=r.history['accuracy'], label='accuracy', ax=ax[1])\n",
    "sns.lineplot(x=range(len(r.history['val_accuracy'])), y=r.history['val_accuracy'], label='val_accuracy', ax=ax[1])\n",
    "ax[0].set_title('Loss')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[0].set_xlabel('Epochs')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ce5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data metrics and confusion matrix\n",
    "model=tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "y_pred = model.predict(valid_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_c)\n",
    "recall_macro = recall_score(y_test, y_pred_c, average='macro')\n",
    "precision_macro = precision_score(y_test, y_pred_c, average='macro')\n",
    "f1DS = f1_score(y_test, y_pred_c, average='macro')\n",
    "\n",
    "print(f\"Validation Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall_macro:.4f}\")\n",
    "print(f\"Precision: {precision_macro:.4f}\")\n",
    "print(f\"F1 Score: {f1DS:.4f}\")\n",
    "\n",
    "target_names = [f'Class {i}' for i in range(model.output_shape[-1])]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_c), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Validation Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889efdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data metrics and confusion matrix\n",
    "y_pred = model.predict(test_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_c)\n",
    "recall_macro = recall_score(y_test, y_pred_c, average='macro')\n",
    "precision_macro = precision_score(y_test, y_pred_c, average='macro')\n",
    "f1DS = f1_score(y_test, y_pred_c, average='macro')\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall_macro:.4f}\")\n",
    "print(f\"Precision: {precision_macro:.4f}\")\n",
    "print(f\"F1 Score: {f1DS:.4f}\")\n",
    "\n",
    "target_names = [f'Class {i}' for i in range(model.output_shape[-1])]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_c), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for validation set\n",
    "\n",
    "y_pred = model.predict(valid_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "n_classes = model.output_shape[-1]\n",
    "binary_y_test = label_binarize(y_test, classes=range(n_classes))\n",
    "\n",
    "tpr = {}\n",
    "fpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(binary_y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,label=f'Class {i} (AUC = {roc_auc[i]:.4f})')\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle='--', lw=2,label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.4f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for test set\n",
    "\n",
    "y_pred = model.predict(test_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "n_classes = model.output_shape[-1]\n",
    "binary_y_test = label_binarize(valid_Y, classes=range(n_classes))\n",
    "\n",
    "tpr = {}\n",
    "fpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(binary_y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,label=f'Class {i} (AUC = {roc_auc[i]:.4f})')\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle='--', lw=2,label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.4f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
