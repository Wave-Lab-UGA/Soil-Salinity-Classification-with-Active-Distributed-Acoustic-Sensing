{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Steven Binder, The Photonics and Soft Robotics Lab, The University of Georgia\n",
    "Date:   08-05-2025\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, os.path\n",
    "import warnings \n",
    "import math\n",
    "import librosa\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense,concatenate, Conv2D, Add, BatchNormalization,SpatialDropout2D, Dropout, Flatten, GlobalAveragePooling2D,MaxPooling2D\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix,accuracy_score,f1_score,roc_curve, auc,classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.regularizers import l2\n",
    "tf.config.optimizer.set_jit(False)  # Enable XLA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_features_and_labels(data,labels):\n",
    "    samples,time,feat= data.shape\n",
    "    reshaped_data = np.zeros((samples * 3, time, 2), dtype=data.dtype)\n",
    "\n",
    "    reshaped_data[0::3, :, 0]=data[:, :, 0]  \n",
    "    reshaped_data[0::3, :, 1]=data[:, :, 1]  \n",
    "    reshaped_data[1::3, :, 0]=data[:, :, 2]  \n",
    "    reshaped_data[1::3, :, 1]=data[:, :, 3]  \n",
    "    reshaped_data[2::3, :, 0] = data[:, :, 4]  \n",
    "    reshaped_data[2::3, :, 1] = data[:, :, 5] \n",
    "\n",
    "    reshaped_labels = np.repeat(labels, 3)\n",
    "\n",
    "    return reshaped_data, reshaped_labels\n",
    "\n",
    "def compute_mel_spectrogram(data, sample_rate, n_mels, n_fft, hop_length):\n",
    "    samples, time, features = data.shape\n",
    "\n",
    "    spectrograms = np.zeros((samples, n_mels, time, features))\n",
    "\n",
    "    for i in range(samples):\n",
    "        for j in range(features):\n",
    "            time_series = data[i, :, j]\n",
    "            if np.max(np.abs(time_series)) > 0:\n",
    "                time_series = time_series / np.max(np.abs(time_series))\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=time_series,\n",
    "                sr=sample_rate,\n",
    "                n_mels=n_mels,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                power=2.0\n",
    "            )\n",
    "            mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            spectrograms[i, :, :, j] = mel_db\n",
    "\n",
    "    return spectrograms\n",
    "\n",
    "def augment_spectrogram(data,gaussian_noise_prob=0.8, gaussian_noise_std=0.1,\n",
    "                       multiplicative_noise_prob=0.5, multiplicative_noise_range=(0.7, 1.3),\n",
    "                       time_shift_prob=0.3, time_shift_max=20):\n",
    "    augmented_data = np.copy(data)\n",
    "    if np.random.random() < gaussian_noise_prob: # Adding Gaussian noise\n",
    "        noise = np.random.normal(0, gaussian_noise_std, size=data.shape)\n",
    "        augmented_data = augmented_data + noise\n",
    "\n",
    "    if np.random.random() < multiplicative_noise_prob: # Adding multiplicative noise\n",
    "        for c in range(data.shape[2]):\n",
    "            scale_factor = np.random.uniform(multiplicative_noise_range[0],multiplicative_noise_range[1])\n",
    "            augmented_data[:, :, c] = augmented_data[:, :, c] * scale_factor\n",
    "\n",
    "    if np.random.random() < time_shift_prob: # Adding time shift\n",
    "        shift_amount = np.random.randint(-time_shift_max, time_shift_max + 1)\n",
    "        if shift_amount != 0:\n",
    "            for c in range(data.shape[2]):\n",
    "                if shift_amount > 0:  \n",
    "                    augmented_data[:, shift_amount:, c] = augmented_data[:, :-shift_amount, c]\n",
    "                    augmented_data[:, :shift_amount, c] = 0\n",
    "                else: \n",
    "                    shift_amount = abs(shift_amount)\n",
    "                    augmented_data[:, :-shift_amount, c] = augmented_data[:, shift_amount:, c]\n",
    "                    augmented_data[:, -shift_amount:, c] = 0\n",
    "    \n",
    "    augmented_data = np.clip(augmented_data, 0, 1)\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "def crop_mel_bands(spectrograms, mel_start=0, mel_end=50):\n",
    "    return spectrograms[:, mel_start:mel_end, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and validation data\n",
    "os.chdir(\"/home/stevenbinder/Desktop/ML\")\n",
    "titles=['D01X.npy','D01Y.npy','D02X.npy','D02Y.npy','D03X.npy','D03Y.npy','D04X.npy','D04Y.npy','D05X.npy','D05Y.npy','D06X.npy','D06Y.npy',\n",
    "        'D07X.npy','D07Y.npy','D08X.npy','D08Y.npy','D09X.npy','D09Y.npy','D10X.npy','D10Y.npy',\n",
    "        'D01X_100.npy','D01Y_100.npy','D02X_100.npy','D02Y_100.npy','D03X_100.npy','D03Y_100.npy','D04X_100.npy','D04Y_100.npy',\n",
    "        'D05X_100.npy','D05Y_100.npy','D06X_100.npy','D06Y_100.npy','D07X_100.npy','D07Y_100.npy','D08X_100.npy','D08Y_100.npy','D09X_100.npy','D09Y_100.npy','D10X_100.npy','D10Y_100.npy',\n",
    "        'D01X_300.npy','D01Y_300.npy','D02X_300.npy','D02Y_300.npy','D03X_300.npy','D03Y_300.npy','D04X_300.npy','D04Y_300.npy',\n",
    "        'D05X_300.npy','D05Y_300.npy','D06X_300.npy','D06Y_300.npy','D07X_300.npy','D07Y_300.npy','D08X_300.npy','D08Y_300.npy','D09X_300.npy','D09Y_300.npy','D10X_300.npy','D10Y_300.npy',\n",
    "        'D01X_750.npy','D01Y_750.npy','D02X_750.npy','D02Y_750.npy','D03X_750.npy','D03Y_750.npy','D04X_750.npy','D04Y_750.npy',\n",
    "        'D05X_750.npy','D05Y_750.npy','D06X_750.npy','D06Y_750.npy','D07X_750.npy','D07Y_750.npy','D08X_750.npy','D08Y_750.npy','D09X_750.npy','D09Y_750.npy','D10X_750.npy','D10Y_750.npy',\n",
    "        'D01X_1000.npy','D01Y_1000.npy','D02X_1000.npy','D02Y_1000.npy','D03X_1000.npy','D03Y_1000.npy','D04X_1000.npy','D04Y_1000.npy',\n",
    "        'D05X_1000.npy','D05Y_1000.npy','D06X_1000.npy','D06Y_1000.npy','D07X_1000.npy','D07Y_1000.npy','D08X_1000.npy','D08Y_1000.npy','D09X_1000.npy','D09Y_1000.npy','D10X_1000.npy','D10Y_1000.npy'\n",
    "        ]\n",
    "\n",
    "\n",
    "sam_rate = 5000  \n",
    "n_mels =256\n",
    "n_fft = 1024     \n",
    "hop_length =512\n",
    "\n",
    "batch_size = 10\n",
    "all_processed_X = []\n",
    "all_processed_Y = []\n",
    "\n",
    "for i in range(0, len(titles), batch_size):\n",
    "    batch_titles = titles[i:i+batch_size]\n",
    "    k = 0\n",
    "    total_X0, total_X1, total_X2 = None, None, None\n",
    "    for file in batch_titles:\n",
    "        if 'X' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                x0 = np.load(f)#[0:4]\n",
    "                x1 = np.load(f)#[0:4] \n",
    "                x2 = np.load(f)#[0:4]\n",
    "                if 'X_300' in file:\n",
    "                    x0 = x0[:,40000:190000,:]\n",
    "                    x1 = x1[:,40000:190000,:]\n",
    "                    x2 = x2[:,40000:190000,:]\n",
    "                if k == 0:\n",
    "                    total_X0 = x0\n",
    "                    total_X1 = x1\n",
    "                    total_X2 = x2\n",
    "                    k += 1\n",
    "                else:\n",
    "                    total_X0 = np.vstack((total_X0, x0))\n",
    "                    total_X1 = np.vstack((total_X1, x1))\n",
    "                    total_X2 = np.vstack((total_X2, x2))\n",
    "                del x0, x1, x2\n",
    "        if 'Y' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                y0 = np.load(f)\n",
    "                y1 = np.load(f)\n",
    "                y2 = np.load(f)\n",
    "                if 'Y_300' in file:\n",
    "                    y0 = y0[40000:190000]\n",
    "                    y1 = y1[40000:190000]\n",
    "                    y2 = y2[40000:190000]\n",
    "    batch_total_X = []\n",
    "    batch_total_Y = []\n",
    "    if total_X0 is not None:\n",
    "        for i in range(total_X0.shape[0]):\n",
    "            batch_total_X.append(total_X0[i,:,:])\n",
    "            batch_total_Y.append(y0[0])\n",
    "\n",
    "        for i in range(total_X1.shape[0]):\n",
    "            batch_total_X.append(total_X1[i,:,:])\n",
    "            batch_total_Y.append(y1[0])\n",
    "\n",
    "        for i in range(total_X2.shape[0]):\n",
    "            batch_total_X.append(total_X2[i,:,:])\n",
    "            batch_total_Y.append(y2[0])\n",
    "\n",
    "    batch_total_X = np.array(batch_total_X)\n",
    "    batch_total_Y = np.array(batch_total_Y)\n",
    "\n",
    "    batch_total_X, batch_total_Y = reshape_features_and_labels(batch_total_X, batch_total_Y)\n",
    "\n",
    "    batch_total_X = compute_mel_spectrogram(batch_total_X, sample_rate=sam_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    batch_totalX = crop_mel_bands(batch_totalX, mel_start=0, mel_end=100)\n",
    "    \n",
    "    all_processed_X.append(batch_total_X)\n",
    "    all_processed_Y.extend(batch_total_Y)\n",
    "\n",
    "    del batch_total_X, batch_total_Y\n",
    "    del total_X0, total_X1, total_X2\n",
    "    gc.collect()\n",
    "\n",
    "final_X = np.concatenate(all_processed_X, axis=0)\n",
    "final_Y = np.array(all_processed_Y)\n",
    "\n",
    "globMax= np.max(final_X)\n",
    "globMin= np.min(final_X)\n",
    "\n",
    "final_X=(final_X - globMin) / (globMax - globMin)\n",
    "print(final_X.shape,final_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "os.chdir(\"/home/stevenbinder/Desktop/ML\")\n",
    "titles=['D11X.npy','D11Y.npy',\n",
    "        'D11X_100.npy','D11Y_100.npy',\n",
    "        'D11X_300.npy','D11Y_300.npy',\n",
    "        'D11X_750.npy','D11Y_750.npy',\n",
    "        'D11X_1000.npy','D11Y_1000.npy',]\n",
    "\n",
    "batch_size = 10\n",
    "all_processed_X = []\n",
    "all_processed_Y = []\n",
    "\n",
    "for i in range(0, len(titles), batch_size):\n",
    "    batch_titles = titles[i:i+batch_size]\n",
    "    k = 0\n",
    "    total_X0, total_X1, total_X2 = None, None, None\n",
    "    for file in batch_titles:\n",
    "        if 'X' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                x0 = np.load(f)#[0:4]\n",
    "                x1 = np.load(f)#[0:4] \n",
    "                x2 = np.load(f)#[0:4]\n",
    "                if 'X_300' in file:\n",
    "                    x0 = x0[:,40000:190000,:]\n",
    "                    x1 = x1[:,40000:190000,:]\n",
    "                    x2 = x2[:,40000:190000,:]\n",
    "                if k == 0:\n",
    "                    total_X0 = x0\n",
    "                    total_X1 = x1\n",
    "                    total_X2 = x2\n",
    "                    k += 1\n",
    "                else:\n",
    "                    total_X0 = np.vstack((total_X0, x0))\n",
    "                    total_X1 = np.vstack((total_X1, x1))\n",
    "                    total_X2 = np.vstack((total_X2, x2))\n",
    "                del x0, x1, x2\n",
    "        if 'Y' in file:\n",
    "            with open(file, 'rb') as f:\n",
    "                y0 = np.load(f)\n",
    "                y1 = np.load(f)\n",
    "                y2 = np.load(f)\n",
    "                if 'Y_300' in file:\n",
    "                    y0 = y0[40000:190000]\n",
    "                    y1 = y1[40000:190000]\n",
    "                    y2 = y2[40000:190000]\n",
    "    batch_total_X = []\n",
    "    batch_total_Y = []\n",
    "\n",
    "    if total_X0 is not None:\n",
    "        for i in range(total_X0.shape[0]):\n",
    "            batch_total_X.append(total_X0[i,:,:])\n",
    "            batch_total_Y.append(y0[0])\n",
    "\n",
    "        for i in range(total_X1.shape[0]):\n",
    "            batch_total_X.append(total_X1[i,:,:])\n",
    "            batch_total_Y.append(y1[0])\n",
    "\n",
    "        for i in range(total_X2.shape[0]):\n",
    "            batch_total_X.append(total_X2[i,:,:])\n",
    "            batch_total_Y.append(y2[0])\n",
    "\n",
    "    batch_total_X = np.array(batch_total_X)\n",
    "    batch_total_Y = np.array(batch_total_Y)\n",
    "\n",
    "    batch_total_X, batch_total_Y = reshape_features_and_labels(batch_total_X, batch_total_Y)\n",
    "\n",
    "    batch_total_X = compute_mel_spectrogram(batch_total_X, sample_rate=sam_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    batch_totalX = crop_mel_bands(batch_totalX, mel_start=0, mel_end=100)\n",
    "    \n",
    "    all_processed_X.append(batch_total_X)\n",
    "    all_processed_Y.extend(batch_total_Y)\n",
    "\n",
    "    del batch_total_X, batch_total_Y\n",
    "    del total_X0, total_X1, total_X2\n",
    "    gc.collect()\n",
    "\n",
    "valid_X = np.concatenate(all_processed_X, axis=0)\n",
    "valid_Y = np.array(all_processed_Y)\n",
    "\n",
    "globMax= np.max(valid_X)\n",
    "globMin= np.min(valid_X)\n",
    "\n",
    "valid_X=(valid_X - globMin) / (globMax - globMin)\n",
    "print(valid_X.shape,valid_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset creation\n",
    "\n",
    "final_X = final_X.astype(np.float32)\n",
    "valid_X = valid_X.astype(np.float32)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_X, final_Y, test_size=0.3, stratify=final_Y)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Augment training data\n",
    "chunk_size = 1000\n",
    "noise_X_train = []\n",
    "\n",
    "for aug_type in range(3):\n",
    "    aug_samples = np.zeros_like(X_train)\n",
    "    \n",
    "    for i in range(0, len(X_train), chunk_size):\n",
    "        chunk = X_train[i:i+chunk_size].copy()\n",
    "\n",
    "        for j in range(len(chunk)):\n",
    "            if aug_type == 0:\n",
    "                aug_samples[i+j] = augment_spectrogram(chunk[j],gaussian_noise_prob=1.0, gaussian_noise_std=0.12,multiplicative_noise_prob=0.0,time_shift_prob=0.0)\n",
    "            elif aug_type == 1:\n",
    "                aug_samples[i+j] = augment_spectrogram(chunk[j],gaussian_noise_prob=0.3, gaussian_noise_std=0.05,multiplicative_noise_prob=1.0, multiplicative_noise_range=(0.8, 1.2), time_shift_prob=0.0)\n",
    "            else:\n",
    "                aug_samples[i+j] = augment_spectrogram(chunk[j],gaussian_noise_prob=0.3, gaussian_noise_std=0.05,multiplicative_noise_prob=0.3,time_shift_prob=1.0, time_shift_max=15)\n",
    "\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    noise_X_train.append(aug_samples)\n",
    "\n",
    "aug_data = np.concatenate(noise_X_train, axis=0)\n",
    "aug_labels = np.tile(y_train, 3)\n",
    "\n",
    "X_train = np.concatenate((X_train, aug_data), axis=0)\n",
    "y_train = np.concatenate((y_train, aug_labels), axis=0)\n",
    "\n",
    "del noise_X_train, aug_data, aug_labels\n",
    "gc.collect()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "train_DS = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "    .cache()\\\n",
    "    .shuffle(1000)\\\n",
    "    .batch(batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_DS = tf.data.Dataset.from_tensor_slices((X_test, y_test))\\\n",
    "    .cache()\\\n",
    "    .batch(batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_DS = tf.data.Dataset.from_tensor_slices((valid_X, valid_Y))\\\n",
    "    .cache()\\\n",
    "    .batch(batch_size)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "del X_train, X_test\n",
    "gc.collect()\n",
    "\n",
    "process = psutil.Process()\n",
    "print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b52a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "input_shape = (100, 293, 1) \n",
    "weight_decay = 1e-4\n",
    "initial_lr = 0.5e-4\n",
    "epochs = 60\n",
    "\n",
    "def basic_block(x, filters, stride=1, wd=1e-4, name=None):\n",
    "    shortcut = x\n",
    "    y = layers.Conv2D(filters, 3, strides=stride, padding='same', use_bias=False,\n",
    "                      kernel_regularizer=l2(wd))(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Conv2D(filters, 3, strides=1, padding='same', use_bias=False,\n",
    "                      kernel_regularizer=l2(wd))(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False,\n",
    "                                 kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    out = layers.Add()([y, shortcut])\n",
    "    out = layers.Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "def build_resnet18(input_shape=(100,293,1), num_classes=3, wd=1e-4):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False,\n",
    "                      kernel_regularizer=l2(wd))(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "\n",
    "    x = basic_block(x, 32,  stride=1, wd=wd)\n",
    "    x = basic_block(x, 32,  stride=1, wd=wd)\n",
    "\n",
    "    x = basic_block(x, 64, stride=2, wd=wd)\n",
    "    x = basic_block(x, 64, stride=1, wd=wd)\n",
    "\n",
    "    x = basic_block(x, 128, stride=2, wd=wd)\n",
    "    x = basic_block(x, 128, stride=1, wd=wd)\n",
    "\n",
    "    x = basic_block(x, 128, stride=2, wd=wd)\n",
    "    x = basic_block(x, 128, stride=1, wd=wd)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return models.Model(inp, out, name='ResNet18')\n",
    "\n",
    "resnet18 = build_resnet18(input_shape=input_shape, num_classes=num_classes, wd=weight_decay)\n",
    "resnet18.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "ckpt = callbacks.ModelCheckpoint('best_resnet18.keras', monitor='val_accuracy',\n",
    "                                 save_best_only=True, mode='max')\n",
    "es = callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "lr_plateau = callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "hist = resnet18.fit(\n",
    "    train_DS,\n",
    "    validation_data=test_DS,\n",
    "    epochs=epochs,\n",
    "    callbacks=[ckpt, es, lr_plateau],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "axes[0].plot(hist.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(hist.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[1].plot(hist.history['loss'], label='Train Loss')\n",
    "axes[1].plot(hist.history['val_loss'], label='Validation Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ce5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data metrics and confusion matrix\n",
    "y_pred = resnet18.predict(valid_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_c)\n",
    "recall_macro = recall_score(y_test, y_pred_c, average='macro')\n",
    "precision_macro = precision_score(y_test, y_pred_c, average='macro')\n",
    "f1DS = f1_score(y_test, y_pred_c, average='macro')\n",
    "\n",
    "print(f\"Validation Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall_macro:.4f}\")\n",
    "print(f\"Precision: {precision_macro:.4f}\")\n",
    "print(f\"F1 Score: {f1DS:.4f}\")\n",
    "\n",
    "target_names = [f'Class {i}' for i in range(resnet18.output_shape[-1])]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_c), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Validation Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889efdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data metrics and confusion matrix\n",
    "y_pred = resnet18.predict(test_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_c)\n",
    "recall_macro = recall_score(y_test, y_pred_c, average='macro')\n",
    "precision_macro = precision_score(y_test, y_pred_c, average='macro')\n",
    "f1DS = f1_score(y_test, y_pred_c, average='macro')\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall_macro:.4f}\")\n",
    "print(f\"Precision: {precision_macro:.4f}\")\n",
    "print(f\"F1 Score: {f1DS:.4f}\")\n",
    "\n",
    "target_names = [f'Class {i}' for i in range(resnet18.output_shape[-1])]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_c), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for validation set\n",
    "\n",
    "y_pred = resnet18.predict(valid_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "n_classes = resnet18.output_shape[-1]\n",
    "binary_y_test = label_binarize(y_test, classes=range(n_classes))\n",
    "\n",
    "tpr = {}\n",
    "fpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(binary_y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,label=f'Class {i} (AUC = {roc_auc[i]:.4f})')\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle='--', lw=2,label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.4f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for test set\n",
    "\n",
    "y_pred = resnet18.predict(test_DS)\n",
    "y_pred_c = np.argmax(y_pred, axis=1)\n",
    "\n",
    "n_classes = resnet18.output_shape[-1]\n",
    "binary_y_test = label_binarize(valid_Y, classes=range(n_classes))\n",
    "\n",
    "tpr = {}\n",
    "fpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(binary_y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,label=f'Class {i} (AUC = {roc_auc[i]:.4f})')\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle='--', lw=2,label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.4f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
